{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c0f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO\n",
    "## transition prob matrix (s,a,s')\n",
    "## reward matrix (s,a)\n",
    "\n",
    "def env(s, a):\n",
    "    '''\n",
    "    args: state dim, action dim\n",
    "\n",
    "    return: transition prob, reward matrix\n",
    "    '''\n",
    "    \n",
    "    p = np.random.random(s*a*s).reshape(s,a,s)\n",
    "    p = p / p.sum(axis=2, keepdims=True) \n",
    "    r = np.random.random(s*a).reshape(s,a)\n",
    "    \n",
    "    return p, r\n",
    "\n",
    "def random_policy(s,a):\n",
    "    '''\n",
    "    args: state dim, action dim\n",
    "\n",
    "    return: random policy\n",
    "    '''\n",
    "    pi = np.random.random(s*a).reshape(s,a)\n",
    "    pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba319a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, action, gamma\n",
    "s= 10\n",
    "a = 10\n",
    "gamma = 0.9\n",
    "\n",
    "# transition prob, reward matrix\n",
    "P,R = env(s,a)\n",
    "\n",
    "# random policy\n",
    "pi = random_policy(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e745d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# linear equation (evaluation)\n",
    "## v = (I - gamma*P)^-1 * r\n",
    "\n",
    "# policy reward, policy transition prob\n",
    "r_pi = np.sum(pi*R, axis=1)\n",
    "p_pi = np.sum(pi[:,:,np.newaxis] * P, axis=1)\n",
    "\n",
    "print(p_pi.shape)\n",
    "print(r_pi.shape)\n",
    "\n",
    "V = np.linalg.inv(np.eye(s) - gamma*p_pi) @ r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8ad1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[5.0012556  5.15941529 5.37829811 5.12676508 5.10051395 5.25804683\n",
      " 5.08696993 5.22286406 4.98872513 5.12514297]\n"
     ]
    }
   ],
   "source": [
    "print(V.shape)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a93bb39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00124579, 5.15940548, 5.3782883 , 5.12675526, 5.10050414,\n",
       "       5.25803702, 5.08696012, 5.22285425, 4.98871532, 5.12513315])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_evaluation_matrix(p, r, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        policy: policy matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: state-value function (S)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    \n",
    "    # Initialize value function\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        # Compute expected rewards for each state (S x 1)\n",
    "        expected_rewards = np.sum(policy * r, axis=1)\n",
    "        \n",
    "        # Compute expected next state values (S x S)\n",
    "        # p_policy: (S x S) matrix where p_policy[s,s'] = sum_a policy(s,a) * p(s,a,s')\n",
    "        p_policy = np.sum(policy[:, :, np.newaxis] * p, axis=1)\n",
    "        \n",
    "        # Compute new value function\n",
    "        V_new = expected_rewards + gamma * np.dot(p_policy, V)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "            \n",
    "        V = V_new\n",
    "        \n",
    "    return V\n",
    "\n",
    "policy_evaluation_matrix(P, R, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84cb72b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_improvement_matrix(p, r, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    policy improvement using \n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        V: current value function (S)\n",
    "        gamma: discount factor\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: improved policy matrix (S x A)\n",
    "    \"\"\"\n",
    "    # Q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) * V(s')\n",
    "    Q = r + gamma * np.sum(p * V[np.newaxis, np.newaxis, :], axis=2)\n",
    "\n",
    "    new_policy = np.zeros_like(Q)\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    new_policy[np.arange(len(Q)), best_actions] = 1\n",
    "    \n",
    "    return new_policy\n",
    "\n",
    "\n",
    "policy_improvement_matrix(P, R, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "811dd79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       " array([9.02434363, 8.99806546, 8.98017772, 8.85479739, 8.96444362,\n",
       "        9.01732508, 8.96768506, 8.96206667, 8.68237136, 8.90715751]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy iteration\n",
    "def policy_iteration_matrix(p, r, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    policy iteration \n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: improved policy matrix (S x A)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        V = policy_evaluation_matrix(p, r, policy, gamma, theta)\n",
    "        new_policy = policy_improvement_matrix(p, r, V, gamma)\n",
    "        \n",
    "        if new_policy.all() == policy.all():\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        \n",
    "    return policy, V\n",
    "\n",
    "policy_iteration_matrix(P, R, pi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f9398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.03717282, 9.01160563, 8.99217841, 8.86719043, 8.99230531,\n",
       "        9.03093301, 8.98070783, 8.97450407, 8.69424636, 8.92052505]),\n",
       " array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration_matrix(p, r, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    value iteration \n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: optimal state-value function (S)\n",
    "        policy: optimal policy (S x A)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    n_actions = p.shape[1]\n",
    "    \n",
    "    # Initialize value function\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        # Q-values for all state-action pairs\n",
    "        ## Q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) * V(s')\n",
    "        Q = r + gamma * np.sum(p * V[np.newaxis, np.newaxis, :], axis=2)\n",
    "        \n",
    "        # new value function \n",
    "        V_new = np.max(Q, axis=1)\n",
    "        \n",
    "        # convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "            \n",
    "        V = V_new\n",
    "    \n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy[np.arange(n_states), best_actions] = 1\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "value_iteration_vectorized(P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf640f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
