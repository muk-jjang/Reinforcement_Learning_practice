{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c0f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# TODO\n",
    "## transition prob matrix (s,a,s')\n",
    "## reward matrix (s,a)\n",
    "## random 환경 구현, random policy 구현, 동기 비동기 for문 구현, matrix 구현, linear equation 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b188d",
   "metadata": {},
   "source": [
    "# Env, Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a59b1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(s, a):\n",
    "    '''\n",
    "    Args: \n",
    "        state dim, action dim\n",
    "\n",
    "    Returns: \n",
    "        transition prob, reward matrix\n",
    "    '''\n",
    "    \n",
    "    p = np.random.random(s*a*s).reshape(s,a,s)\n",
    "    p = p / p.sum(axis=2, keepdims=True) \n",
    "    r = np.random.random(s*a).reshape(s,a)\n",
    "    \n",
    "    return p, r\n",
    "\n",
    "# class Env:\n",
    "#     def __init__(self, s, a):\n",
    "#         self.state_dim = s\n",
    "#         self.action_dim = a\n",
    "#         self.current_state = 0  # 초기 상태\n",
    "#         self._generate_tp_(s, a)\n",
    "#         self._generate_r_(s, a)\n",
    "    \n",
    "#     def _generate_tp_(self, s, a):\n",
    "#         self.p = np.random.random(s*a*s).reshape(s,a,s)\n",
    "#         self.p = self.p / self.p.sum(axis=2, keepdims=True) \n",
    "    \n",
    "#     def _generate_r_(self, s, a):\n",
    "#         self.r = np.random.random(s*a).reshape(s,a)\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         next_state_probs = self.p[self.current_state, action]\n",
    "#         next_state = np.random.choice(self.state_dim, p=next_state_probs)\n",
    "#         reward = self.r[self.current_state, action]\n",
    "#         self.current_state = next_state\n",
    "#         return next_state, reward\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.current_state = 0\n",
    "#         return self.current_state\n",
    "    \n",
    "#     def get_dynamics(self):\n",
    "#         return self.p, self.r\n",
    "        \n",
    "\n",
    "def random_policy(s,a):\n",
    "    '''\n",
    "    Args: \n",
    "        state dim, action dim\n",
    "\n",
    "    Returns: \n",
    "        random policy\n",
    "    '''\n",
    "    pi = np.random.random(s*a).reshape(s,a)\n",
    "    pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea171d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "(10, 5, 10)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "s,a = 10, 5\n",
    "P, R = env(s,a)\n",
    "# env = Env(s,a)\n",
    "# P,R = env.get_dynamics()\n",
    "policy = random_policy(s,a)\n",
    "print(policy.shape)\n",
    "print(P.shape)\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca38f8f",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab74e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear equation(evaluation)\n",
    "def linear_equation(P,R, policy, gamma):\n",
    "    r_pi = np.sum(policy*R, axis=1)\n",
    "    p_pi = np.sum(policy[:, :,np.newaxis] * P, axis=1)\n",
    "    V = np.linalg.inv(np.eye(s) - gamma*p_pi) @ r_pi\n",
    "    return V\n",
    "\n",
    "# policy evaluation matrix\n",
    "def policy_evaluation_matrix(p, r, policy, env=None, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        policy: policy matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: state-value function (S)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    \n",
    "    # Initialize value function or random\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        # Compute expected rewards for each state (S x 1)\n",
    "        expected_rewards = np.sum(policy * r, axis=1)\n",
    "        \n",
    "        # Compute expected next state values (S x S)\n",
    "        # p_policy: (S x S) matrix where p_policy[s,s'] = sum_a policy(s,a) * p(s,a,s')\n",
    "        p_policy = np.sum(policy[:, :, np.newaxis] * p, axis=1)\n",
    "        \n",
    "        # Compute new value function\n",
    "        V_new = expected_rewards + gamma * np.dot(p_policy, V)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "            \n",
    "        V = V_new\n",
    "        \n",
    "    return V\n",
    "\n",
    "# policy evaluation for iteration sync\n",
    "def policy_evaluation_for_sync(p, r, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Policy evaluation using for loops\n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        policy: policy matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: state-value function (S)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    n_actions = p.shape[1]\n",
    "    \n",
    "    # Initialize value function\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.zeros(n_states)\n",
    "        \n",
    "        # For each state\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            total = 0\n",
    "            \n",
    "            # For each action\n",
    "            for a in range(n_actions):\n",
    "                # Calculate expected reward\n",
    "                reward = r[s, a]\n",
    "                \n",
    "                # Calculate expected next state value\n",
    "                next_state_value = 0\n",
    "                for s_prime in range(n_states):\n",
    "                    next_state_value+= p[s, a, s_prime] * V[s_prime]\n",
    "                \n",
    "                # Add to total with policy probability\n",
    "                total += policy[s, a] * (reward + gamma * next_state_value)\n",
    "            \n",
    "            V_new[s] = total\n",
    "            delta = max(delta, abs(v - V_new[s]))\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return V\n",
    "\n",
    "# policy evaluation for iteration async\n",
    "def policy_evaluation_for_async(p, r, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Policy evaluation using for loops async\n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        policy: policy matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: state-value function (S)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    n_actions = p.shape[1]\n",
    "\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        delta=0\n",
    "\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            total = 0\n",
    "\n",
    "            for a in range(n_actions):\n",
    "                reward = r[s,a]\n",
    "\n",
    "                next_state_value = 0\n",
    "                for s_prime in range(n_states):\n",
    "                    next_state_value += p[s,a,s_prime] * V[s_prime]\n",
    "                \n",
    "                total += policy[s,a] * (reward + gamma * next_state_value)\n",
    "\n",
    "            V[s] = total\n",
    "            delta = max(delta,abs(v-V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b094a7",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0883df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement matrix\n",
    "def policy_improvement_matrix(p, r, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    policy improvement \n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        V: current value function (S)\n",
    "        gamma: discount factor\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: improved policy matrix (S x A)\n",
    "    \"\"\"\n",
    "    # Q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) * V(s')\n",
    "    Q = r + gamma * np.sum(p * V[np.newaxis, np.newaxis, :], axis=2)\n",
    "\n",
    "    new_policy = np.zeros_like(Q)\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    new_policy[np.arange(len(Q)), best_actions] = 1\n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1cc1c",
   "metadata": {},
   "source": [
    "## policy iteration, value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aaace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "def policy_iteration_matrix(p, r, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    policy iteration \n",
    "    \n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: threshold\n",
    "        \n",
    "    Returns:\n",
    "        new_policy: improved policy matrix (S x A)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        V = policy_evaluation_matrix(p, r, policy, gamma, theta)\n",
    "        new_policy = policy_improvement_matrix(p, r, V, gamma)\n",
    "        \n",
    "        if new_policy.all() == policy.all():\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        \n",
    "    return policy, V\n",
    "\n",
    "# value iteration\n",
    "def value_iteration_matrix(p, r, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: transition probability matrix (S x A x S)\n",
    "        r: reward matrix (S x A)\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: optimal state-value function (S)\n",
    "        policy: optimal policy (S x A)\n",
    "    \"\"\"\n",
    "    n_states = p.shape[0]\n",
    "    n_actions = p.shape[1]\n",
    "    \n",
    "    # Initialize value function\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        # Q-values for all state-action pairs\n",
    "        ## Q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) * V(s')\n",
    "        Q = r + gamma * np.sum(p * V[np.newaxis, np.newaxis, :], axis=2)\n",
    "        \n",
    "        # new value function \n",
    "        V_new = np.max(Q, axis=1)\n",
    "        \n",
    "        # convergence\n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "            \n",
    "        V = V_new\n",
    "    \n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy[np.arange(n_states), best_actions] = 1\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba319a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, action, gamma\n",
    "s= 10\n",
    "a = 5\n",
    "gamma = 0.9\n",
    "\n",
    "# transition prob, reward matrix\n",
    "P,R = env(s,a)\n",
    "\n",
    "# random policy\n",
    "policy = random_policy(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cedbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
